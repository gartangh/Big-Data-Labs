{"paragraphs":[{"text":"%md\n# Notebook 1: Flink Introduction\n\n## 1. Concepts\nApache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams.","user":"anonymous","dateUpdated":"2020-03-13T13:06:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Notebook 1: Flink Introduction</h1>\n<h2>1. Concepts</h2>\n<p>Apache Flink is a framework and distributed processing engine for stateful computations over unbounded and bounded data streams.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229233_1323970104","id":"paragraph_1562833436450_654020715","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:32+0000","dateFinished":"2020-03-13T13:06:32+0000","status":"FINISHED","focus":true,"$$hashKey":"object:9568"},{"text":"%md\n### 1.1. Dataflow Programming Model\nThe basic building blocks of Flink programs are **streams** and **transformations**.\n- A **stream** is a un-bounded flow of data records (as opposition to a table which is a bounded flow of records)\n- A **transformation** is an operation that takes one or more streams as input and produces one or more output streams as a result\nTypically, a stream is built out of one or more **sources** (e.g. reading data from some files or a collection, from Apache Kafka, …), goes through some transformations, and ends up in one or **more sinks** (e.g. writing data to file, to stdout, to Kafka, …). More info on sources and sinks can be found at streaming connectors.\nStreams and transformation operators are mapped into **streaming dataflows**, whereas each dataflow starts with one or more sources and ends in one or more sinks. The dataflows resemble arbitrary **DAGs** ( **D**irected **A**cyclic **G**raphs).\n\n|![pic1](http://193.190.127.227/img/intro/pic1)| \n|:--:| \n|*Fig. 1: Example of a Flink program and its built DAG ([Source](https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/programming-model.html): Flink doc.).*|\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>1.1. Dataflow Programming Model</h3>\n<p>The basic building blocks of Flink programs are <strong>streams</strong> and <strong>transformations</strong>.<br/>- A <strong>stream</strong> is a un-bounded flow of data records (as opposition to a table which is a bounded flow of records)<br/>- A <strong>transformation</strong> is an operation that takes one or more streams as input and produces one or more output streams as a result<br/>Typically, a stream is built out of one or more <strong>sources</strong> (e.g. reading data from some files or a collection, from Apache Kafka, …), goes through some transformations, and ends up in one or <strong>more sinks</strong> (e.g. writing data to file, to stdout, to Kafka, …). More info on sources and sinks can be found at streaming connectors.<br/>Streams and transformation operators are mapped into <strong>streaming dataflows</strong>, whereas each dataflow starts with one or more sources and ends in one or more sinks. The dataflows resemble arbitrary <strong>DAGs</strong> ( <strong>D</strong>irected <strong>A</strong>cyclic <strong>G</strong>raphs).</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic1\" alt=\"pic1\" /></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 1: Example of a Flink program and its built DAG (<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/programming-model.html\">Source</a>: Flink doc.).</em></td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229234_-1013920389","id":"paragraph_1562833458271_-249515849","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:33+0000","dateFinished":"2020-03-13T13:06:33+0000","status":"FINISHED","$$hashKey":"object:9569"},{"text":"%md\n### Flink Architecture Overview\n| ![pic2](http://193.190.127.227/img/intro/pic2) | \n|:--:| \n| *Fig. 2: Flink architecture overview ([Source](https://ci.apache.org/projects/flink/flink-docs-release-1.1/internals/general_arch.html): Flink doc.).* |\n\n#### APIs\nFlink offers a set of 3 APIs **(1) the DataSet API** used for batch processing, **(2) the DataStream API** for stream processing and **(3) the Table API** allows the user to write simple SQL queries in high layers of Flink.\n#### Windowing\nit’s when aggregations (sum, count) on unbounded streams (unlimited events) are scoped by **windows** such as “count over the last 5 minutes” (time-driven window) or “sum of the last 100 elements” (data-driven window).  \n#### Time notion\nFor a streaming program, one can refer to 3 different notions of time:\n\n| ![pic3](http://193.190.127.227/img/intro/pic3) | \n|:--:| \n| *Fig. 3: Flink’s 3 different notions of time([Source](https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/programming-model.html#time): Flink doc.).* |\n\n- **Processing time:** refers to the system time of the machine that is executing the respective operation\n- **Event time:** refers to the time when the event occurred on the producing device \n- **Ingestion time:** refers to the time that events enter Flink at the source operator\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Flink Architecture Overview</h3>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic2\" alt=\"pic2\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 2: Flink architecture overview (<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.1/internals/general_arch.html\">Source</a>: Flink doc.).</em> </td>\n    </tr>\n  </tbody>\n</table>\n<h4>APIs</h4>\n<p>Flink offers a set of 3 APIs <strong>(1) the DataSet API</strong> used for batch processing, <strong>(2) the DataStream API</strong> for stream processing and <strong>(3) the Table API</strong> allows the user to write simple SQL queries in high layers of Flink.</p>\n<h4>Windowing</h4>\n<p>it’s when aggregations (sum, count) on unbounded streams (unlimited events) are scoped by <strong>windows</strong> such as “count over the last 5 minutes” (time-driven window) or “sum of the last 100 elements” (data-driven window). </p>\n<h4>Time notion</h4>\n<p>For a streaming program, one can refer to 3 different notions of time:</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic3\" alt=\"pic3\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 3: Flink’s 3 different notions of time(<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/programming-model.html#time\">Source</a>: Flink doc.).</em> </td>\n    </tr>\n  </tbody>\n</table>\n<ul>\n  <li><strong>Processing time:</strong> refers to the system time of the machine that is executing the respective operation</li>\n  <li><strong>Event time:</strong> refers to the time when the event occurred on the producing device</li>\n  <li><strong>Ingestion time:</strong> refers to the time that events enter Flink at the source operator</li>\n</ul>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229234_2118284292","id":"paragraph_1564476195600_-70559880","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:33+0000","dateFinished":"2020-03-13T13:06:33+0000","status":"FINISHED","$$hashKey":"object:9570"},{"text":"%md\n#### State Management\n**Stateful Operations** keep information across multiple events (for example window operators). The state of such stateful operations is maintained in an embedded Key/Value store. The state is partitioned and distributed across the nodes. Accessing to the Key/Value state is only possible on keyed streams, after a keyBy() function.\n\n| ![pic4](http://193.190.127.227/img/intro/pic4) | \n|:--:| \n| *Fig. 4: Stateful operations([Source](https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/programming-model.html#stateful-operations): Flink doc.).* |\n\n- Flink implements fault tolerance using a combination of **stream replay** and **checkpointing**. A **checkpoint** is related to a specific point in each of the input streams along with the corresponding **state** for each of the operators. \n- A streaming dataflow can then be resumed from a checkpoint by restoring the state of the operators and replaying the events from the point of the checkpoint.\n- **Savepoints** are manually triggered checkpoints, which take a snapshot of the program and write it out to a state backend (cfr. 2. Distributed runtime). They rely on the regular checkpointing mechanism for this. Any program writing in the DataStream API can resume execution from a savepoint.\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:33+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>State Management</h4>\n<p><strong>Stateful Operations</strong> keep information across multiple events (for example window operators). The state of such stateful operations is maintained in an embedded Key/Value store. The state is partitioned and distributed across the nodes. Accessing to the Key/Value state is only possible on keyed streams, after a keyBy() function.</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic4\" alt=\"pic4\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 4: Stateful operations(<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/programming-model.html#stateful-operations\">Source</a>: Flink doc.).</em> </td>\n    </tr>\n  </tbody>\n</table>\n<ul>\n  <li>Flink implements fault tolerance using a combination of <strong>stream replay</strong> and <strong>checkpointing</strong>. A <strong>checkpoint</strong> is related to a specific point in each of the input streams along with the corresponding <strong>state</strong> for each of the operators.</li>\n  <li>A streaming dataflow can then be resumed from a checkpoint by restoring the state of the operators and replaying the events from the point of the checkpoint.</li>\n  <li><strong>Savepoints</strong> are manually triggered checkpoints, which take a snapshot of the program and write it out to a state backend (cfr. 2. Distributed runtime). They rely on the regular checkpointing mechanism for this. Any program writing in the DataStream API can resume execution from a savepoint.</li>\n</ul>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229235_-890014545","id":"paragraph_1564478765015_1924381374","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:34+0000","dateFinished":"2020-03-13T13:06:34+0000","status":"FINISHED","$$hashKey":"object:9571"},{"text":"%md\n### 1.2. Distributed runtime environment\n#### Task and operator chains\nFor distributed execution, Flink chains operator sub-tasks together into **tasks**. Each task is executed by one thread. Chaining operators together into tasks is an optimization: it reduces the overhead of thread-to-thread handover and buffering, and **increases** overall **throughput** while **decreasing latency**. \nThe sample dataflow in the figure below is executed with 5 subtasks, and hence with 5 parallel threads:\n\n| ![pic5](http://193.190.127.227/img/intro/pic5) | \n|:--:| \n| *Fig. 5:  Tasks and operator chains([Source](https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/runtime.html#tasks-and-operator-chains): Flink doc.).* |","user":"anonymous","dateUpdated":"2020-03-13T13:06:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>1.2. Distributed runtime environment</h3>\n<h4>Task and operator chains</h4>\n<p>For distributed execution, Flink chains operator sub-tasks together into <strong>tasks</strong>. Each task is executed by one thread. Chaining operators together into tasks is an optimization: it reduces the overhead of thread-to-thread handover and buffering, and <strong>increases</strong> overall <strong>throughput</strong> while <strong>decreasing latency</strong>.<br/>The sample dataflow in the figure below is executed with 5 subtasks, and hence with 5 parallel threads:</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic5\" alt=\"pic5\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 5: Tasks and operator chains(<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/runtime.html#tasks-and-operator-chains\">Source</a>: Flink doc.).</em> </td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229235_38509386","id":"paragraph_1564479565025_466811288","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:34+0000","dateFinished":"2020-03-13T13:06:34+0000","status":"FINISHED","$$hashKey":"object:9572"},{"text":"%md\n####  Runtime = 2 types of processes\nFlink has the **master** and **slaves’ structure**:\n\n- The **JobManager (“Master”)** is the coordinator of the distributed execution. It schedule tasks, coordinate checkpoints, coordinate recovery on failures, etc. There is always at least 1 JobManager. A high-availability setup will have multiple JobManagers, one of which one is always the leader, and the others are standby.\n- The **TaskManager (“Slave”)** executes operators that produce stream (= execute the tasks, or more specifically, the subtasks), deliver their statuses to Job Manager, and exchange the data streams between operators (there must always be at least 1 TaskManager).\n\n| ![pic6](http://193.190.127.227/img/intro/pic6) | \n|:--:| \n| *Fig. 6: Overview of Flink’s distributed execution environment([Source](https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/runtime.html#job-managers-task-managers-clients): Flink doc.).* |\n\n","user":"anonymous","dateUpdated":"2020-03-13T13:08:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Runtime = 2 types of processes</h4>\n<p>Flink has the <strong>master</strong> and <strong>slaves’ structure</strong>:</p>\n<ul>\n  <li>The <strong>JobManager (“Master”)</strong> is the coordinator of the distributed execution. It schedule tasks, coordinate checkpoints, coordinate recovery on failures, etc. There is always at least 1 JobManager. A high-availability setup will have multiple JobManagers, one of which one is always the leader, and the others are standby.</li>\n  <li>The <strong>TaskManager (“Slave”)</strong> executes operators that produce stream (= execute the tasks, or more specifically, the subtasks), deliver their statuses to Job Manager, and exchange the data streams between operators (there must always be at least 1 TaskManager).</li>\n</ul>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic6\" alt=\"pic6\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 6: Overview of Flink’s distributed execution environment(<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/concepts/runtime.html#job-managers-task-managers-clients\">Source</a>: Flink doc.).</em> </td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229235_-422196480","id":"paragraph_1564480707523_1890532560","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:08:21+0000","dateFinished":"2020-03-13T13:08:21+0000","status":"FINISHED","$$hashKey":"object:9573"},{"text":"%md\n### 2. Flink Java Basics\nIndeed, Flink supports two modes of operation: batch and streaming. A batch is simply considered as a finite stream. Behind the doors, Flink handles it as a datastream, as it is its basic building block. Why would one pick one mode over the other? It simply **depends on one’s data source**: \nIf one is dealing with a finite datasource, one might want to use the **batch mode** with the **DataSet API**.\nOn the other hand, if one is dealing with unbounded datastream in real-time, one might want to use the **Streaming mode** with the **DataStream API**.","user":"anonymous","dateUpdated":"2020-03-13T13:06:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>2. Flink Java Basics</h3>\n<p>Indeed, Flink supports two modes of operation: batch and streaming. A batch is simply considered as a finite stream. Behind the doors, Flink handles it as a datastream, as it is its basic building block. Why would one pick one mode over the other? It simply <strong>depends on one’s data source</strong>:<br/>If one is dealing with a finite datasource, one might want to use the <strong>batch mode</strong> with the <strong>DataSet API</strong>.<br/>On the other hand, if one is dealing with unbounded datastream in real-time, one might want to use the <strong>Streaming mode</strong> with the <strong>DataStream API</strong>.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229236_-341123289","id":"paragraph_1564480916668_-989783276","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:35+0000","dateFinished":"2020-03-13T13:06:35+0000","status":"FINISHED","$$hashKey":"object:9574"},{"text":"%md\n#### 2.1. Batch mode (DataSet API)\nSimilarly to Spark, the first step composing our Flink program is to give it its execution context. This is done with the [ExecutionEnvironment](https://ci.apache.org/projects/flink/flink-docs-master/api/java/org/apache/flink/api/java/ExecutionEnvironment.html) class.\nTo create an execution environment, do the following:\n    \n    ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n\nBe aware that when we launch the Flink application on our local machine it will perform processing on the local JVM. If we want to start processing data on a cluster of machines, we’d need to install Flink on those machines and configure the ExecutionEnvironment accordingly .\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:35+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>2.1. Batch mode (DataSet API)</h4>\n<p>Similarly to Spark, the first step composing our Flink program is to give it its execution context. This is done with the <a href=\"https://ci.apache.org/projects/flink/flink-docs-master/api/java/org/apache/flink/api/java/ExecutionEnvironment.html\">ExecutionEnvironment</a> class.<br/>To create an execution environment, do the following:</p>\n<pre><code>ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();\n</code></pre>\n<p>Be aware that when we launch the Flink application on our local machine it will perform processing on the local JVM. If we want to start processing data on a cluster of machines, we’d need to install Flink on those machines and configure the ExecutionEnvironment accordingly .</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229236_989181623","id":"paragraph_1564480916508_-1933485095","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:36+0000","dateFinished":"2020-03-13T13:06:36+0000","status":"FINISHED","$$hashKey":"object:9575"},{"text":"%md\n##### 2.1.1. Creating a DataSet\nTo start performing transformations, we need to supply our program with some input data. One can create a DataSet from various multiple sources (see [Flink batch connectors](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/batch/connectors.html)).\nLet’s say we want to create a DataSet of numbers (integers):\n    \n    DataSet<Integer> amounts = env.fromElements(1, 29, 40, 50);","user":"anonymous","dateUpdated":"2020-03-13T13:06:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h5>2.1.1. Creating a DataSet</h5>\n<p>To start performing transformations, we need to supply our program with some input data. One can create a DataSet from various multiple sources (see <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/batch/connectors.html\">Flink batch connectors</a>).<br/>Let’s say we want to create a DataSet of numbers (integers):</p>\n<pre><code>DataSet&lt;Integer&gt; amounts = env.fromElements(1, 29, 40, 50);\n</code></pre>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229236_1654073358","id":"paragraph_1564480916365_-672164454","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:36+0000","dateFinished":"2020-03-13T13:06:36+0000","status":"FINISHED","$$hashKey":"object:9576"},{"text":"%md\n##### 2.1.2. Transformations\nOnce we instantiate a DataSet,  we can apply transformations on it. This is done with the help of operators. Here follows the most frequently used.\nThe complete list can be found at [Batch transformations](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/batch/dataset_transformations.html).","user":"anonymous","dateUpdated":"2020-03-13T13:06:36+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h5>2.1.2. Transformations</h5>\n<p>Once we instantiate a DataSet, we can apply transformations on it. This is done with the help of operators. Here follows the most frequently used.<br/>The complete list can be found at <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/batch/dataset_transformations.html\">Batch transformations</a>.</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229236_-1062022171","id":"paragraph_1564480916239_1571190630","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:37+0000","dateFinished":"2020-03-13T13:06:37+0000","status":"FINISHED","$$hashKey":"object:9577"},{"text":"%md\n###### a. Filter / reduce\n[Filter](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/batch/dataset_transformations.html#filter) / [reduce](https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/dataset_transformations.html#reduce-on-full-dataset) transformation documentation.\n\nLet’s say we want to:\n- 1. Filter numbers that are above a certain threshold\n- 2. Sum them up\nThis is performed with **filter()** and **reduce()** operations:\n\n    int threshold = 30;\n    List<Integer> collect = amounts\n        .filter(a -> a > threshold)\n        .reduce((i1, i2) -> i1 + i2)\n        .collect();\n\nOne could notice the **collect()** method called at the end of the filterReduce transformation. This is done in order to trigger the computation. Indeed, Flink **transformations** are **lazy**, meaning that they are not executed until a **sink** operation is invoked. collect() is indeed one of those sink operations (such as count() or print()).\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h6>a. Filter / reduce</h6>\n<p><a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/batch/dataset_transformations.html#filter\">Filter</a> / <a href=\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/dataset_transformations.html#reduce-on-full-dataset\">reduce</a> transformation documentation.</p>\n<p>Let’s say we want to:<br/>- 1. Filter numbers that are above a certain threshold<br/>- 2. Sum them up<br/>This is performed with <strong>filter()</strong> and <strong>reduce()</strong> operations:</p>\n<pre><code>int threshold = 30;\nList&lt;Integer&gt; collect = amounts\n    .filter(a -&gt; a &gt; threshold)\n    .reduce((i1, i2) -&gt; i1 + i2)\n    .collect();\n</code></pre>\n<p>One could notice the <strong>collect()</strong> method called at the end of the filterReduce transformation. This is done in order to trigger the computation. Indeed, Flink <strong>transformations</strong> are <strong>lazy</strong>, meaning that they are not executed until a <strong>sink</strong> operation is invoked. collect() is indeed one of those sink operations (such as count() or print()).</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229236_1562322857","id":"paragraph_1564481865489_-69797533","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:37+0000","dateFinished":"2020-03-13T13:06:37+0000","status":"FINISHED","$$hashKey":"object:9578"},{"text":"%md\n###### b. Map\n[Map](https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/dataset_transformations.html#map) transformation documentation.\n\nLet’s say that we some Person objects:\t\n\n    class Person {\n        private int age;\n        private String name;\n        \n        Person(int age, String name) {\n            this.age = age;\n            this.name = name;\n        }\n    }\n    \nAnd we want to create a DataSet out of these objects:\n    \n    DataSet<Person> personDataSource = env.fromCollection(\n        Arrays.asList(\n            new Person(23, \"Mike\"),\n            new Person(75, \"Edward\")));\n\nLet’s say we want to extract only the age field from every object of the collection. We would do that with the **map()** operator:\n\n    List<Integer> ages = personDataSource\n        .map(p -> p.age)\n        .collect();\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h6>b. Map</h6>\n<p><a href=\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/dataset_transformations.html#map\">Map</a> transformation documentation.</p>\n<p>Let’s say that we some Person objects: </p>\n<pre><code>class Person {\n    private int age;\n    private String name;\n\n    Person(int age, String name) {\n        this.age = age;\n        this.name = name;\n    }\n}\n</code></pre>\n<p>And we want to create a DataSet out of these objects:</p>\n<pre><code>DataSet&lt;Person&gt; personDataSource = env.fromCollection(\n    Arrays.asList(\n        new Person(23, &quot;Mike&quot;),\n        new Person(75, &quot;Edward&quot;)));\n</code></pre>\n<p>Let’s say we want to extract only the age field from every object of the collection. We would do that with the <strong>map()</strong> operator:</p>\n<pre><code>List&lt;Integer&gt; ages = personDataSource\n    .map(p -&gt; p.age)\n    .collect();\n</code></pre>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229236_2030889940","id":"paragraph_1564481865316_-1119500302","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:38+0000","dateFinished":"2020-03-13T13:06:38+0000","status":"FINISHED","$$hashKey":"object:9579"},{"text":"%md\n###### c. Join\n[Join](https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/dataset_transformations.html#join) transformation documentation.\n\nWe also might want to join two DataSets on some id field. We would do that with the **join()** operator. Let’s say we want to create collections of transactions and addresses of some user:\n\n    //Addresses\n    Tuple3<Integer, String, String> address \n        = new Tuple3<>(1, \"5th Avenue\", \"London\");\n    DataSet<Tuple3<Integer, String, String>> addresses \n        = env.fromElements(address);\n\n    //Transactions\n    Tuple2<Integer, String> firstTransaction \n        = new Tuple2<>(1, \"Transaction_1\");\n    DataSet<Tuple2<Integer, String>> transactions \n     = env.fromElements(firstTransaction, new Tuple2<>(12, \"Transaction_2\"));\n\nTo join those DataSets on the id field, we need to implement a [KeySelector interface](https://ci.apache.org/projects/flink/flink-docs-master/api/java/org/apache/flink/api/java/functions/KeySelector.html) for address and transaction. KeySelector has the following syntax:\n<center> **Interface KeySelector`<IN,KEY>`** </center>\n**where:**\n- IN - Type of objects to extract the key from.\n- KEY - Type of key.\n**We will give it:**\n\n- For the transactions:\n    -   IN - a Tuple2<Integer: id, String: transaction name> (i.e. a transaction)\n    -   KEY - an Integer (i.e. the “id” field)\n    \n- For the addresses:\n    -   IN - a Tuple3<Integer: id, String: street, String: city> (i.e. an address)\n    -   KEY - an Integer (i.e. the “id” field).\n\n.\n\n    class IdKeySelectorTransaction implements KeySelector<Tuple2<Integer, String>, Integer> {\n        @Override\n        public Integer getKey(Tuple2<Integer, String> value) {\n            //Returns the field 0 (=id) of the transaction Tuple\n            return value.f0;\n        }\n    }\n        \n    class IdKeySelectorAddress implements KeySelector<Tuple3<Integer, String, String>, Integer> {\n        @Override\n        public Integer getKey(Tuple3<Integer, String, String> value) {\n            //Returns the field 0 (=id) of the address Tuple\n            return value.f0;\n        }\n    }\n    \nEach selector is only returning the field on which the join should be performed. We can now implement the join per-se using the selectors we just defined. Let’s store the join results in a new Tuple2 made of out of a transaction Tuple2 and an address Tuple3:\n\n    List<Tuple2<Tuple2<Integer, String>, Tuple3<Integer, String, String>>>joined = transactions.join(addresses)\n        .where(new IdKeySelectorTransaction())\n        .equalTo(new IdKeySelectorAddress())\n        .collect();","user":"anonymous","dateUpdated":"2020-03-13T13:06:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h6>c. Join</h6>\n<p><a href=\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/dataset_transformations.html#join\">Join</a> transformation documentation.</p>\n<p>We also might want to join two DataSets on some id field. We would do that with the <strong>join()</strong> operator. Let’s say we want to create collections of transactions and addresses of some user:</p>\n<pre><code>//Addresses\nTuple3&lt;Integer, String, String&gt; address \n    = new Tuple3&lt;&gt;(1, &quot;5th Avenue&quot;, &quot;London&quot;);\nDataSet&lt;Tuple3&lt;Integer, String, String&gt;&gt; addresses \n    = env.fromElements(address);\n\n//Transactions\nTuple2&lt;Integer, String&gt; firstTransaction \n    = new Tuple2&lt;&gt;(1, &quot;Transaction_1&quot;);\nDataSet&lt;Tuple2&lt;Integer, String&gt;&gt; transactions \n = env.fromElements(firstTransaction, new Tuple2&lt;&gt;(12, &quot;Transaction_2&quot;));\n</code></pre>\n<p>To join those DataSets on the id field, we need to implement a <a href=\"https://ci.apache.org/projects/flink/flink-docs-master/api/java/org/apache/flink/api/java/functions/KeySelector.html\">KeySelector interface</a> for address and transaction. KeySelector has the following syntax:<br/><center> <strong>Interface KeySelector<code>&lt;IN,KEY&gt;</code></strong> </center><br/><strong>where:</strong><br/>- IN - Type of objects to extract the key from.<br/>- KEY - Type of key.<br/><strong>We will give it:</strong></p>\n<ul>\n  <li>\n    <p>For the transactions:</p>\n    <ul>\n      <li>IN - a Tuple2&lt;Integer: id, String: transaction name&gt; (i.e. a transaction)</li>\n      <li>KEY - an Integer (i.e. the “id” field)</li>\n    </ul>\n  </li>\n  <li>\n    <p>For the addresses:</p>\n    <ul>\n      <li>IN - a Tuple3&lt;Integer: id, String: street, String: city&gt; (i.e. an address)</li>\n      <li>KEY - an Integer (i.e. the “id” field).</li>\n    </ul>\n  </li>\n</ul>\n<p>.</p>\n<pre><code>class IdKeySelectorTransaction implements KeySelector&lt;Tuple2&lt;Integer, String&gt;, Integer&gt; {\n    @Override\n    public Integer getKey(Tuple2&lt;Integer, String&gt; value) {\n        //Returns the field 0 (=id) of the transaction Tuple\n        return value.f0;\n    }\n}\n\nclass IdKeySelectorAddress implements KeySelector&lt;Tuple3&lt;Integer, String, String&gt;, Integer&gt; {\n    @Override\n    public Integer getKey(Tuple3&lt;Integer, String, String&gt; value) {\n        //Returns the field 0 (=id) of the address Tuple\n        return value.f0;\n    }\n}\n</code></pre>\n<p>Each selector is only returning the field on which the join should be performed. We can now implement the join per-se using the selectors we just defined. Let’s store the join results in a new Tuple2 made of out of a transaction Tuple2 and an address Tuple3:</p>\n<pre><code>List&lt;Tuple2&lt;Tuple2&lt;Integer, String&gt;, Tuple3&lt;Integer, String, String&gt;&gt;&gt;joined = transactions.join(addresses)\n    .where(new IdKeySelectorTransaction())\n    .equalTo(new IdKeySelectorAddress())\n    .collect();\n</code></pre>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229237_-769510125","id":"paragraph_1564481865056_-1036250890","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:38+0000","dateFinished":"2020-03-13T13:06:38+0000","status":"FINISHED","$$hashKey":"object:9580"},{"text":"%md\n###### d. Sort\n[Sort partition](https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/) transformation documentation.\n\nLet’s say we have the following collection of Tuple2:\n\n    Tuple2<Integer, String> secondPerson = new Tuple2<>(4, \"Gautier\");\n    Tuple2<Integer, String> thirdPerson = new Tuple2<>(5, \"Frédéric\");\n    Tuple2<Integer, String> fourthPerson = new Tuple2<>(200, \"Cyrille\");\n    Tuple2<Integer, String> firstPerson = new Tuple2<>(1, \"Charles\");\n    DataSet<Tuple2<Integer, String>> transactions = env.fromElements(fourthPerson, secondPerson, thirdPerson, firstPerson);\n    \nIf we want to sort this collection by the first field of the tuple i.e. the id field, we’d do that with the **sortPartitions()** operator (and the IdKeySelectorTransaction we implemented a bit earlier):\n\n    List<Tuple2<Integer, String>> sorted = transactions\n        .sortPartition(new IdKeySelectorTransaction(), Order.ASCENDING)\n        .collect();","user":"anonymous","dateUpdated":"2020-03-13T13:06:38+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h6>d. Sort</h6>\n<p><a href=\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/batch/\">Sort partition</a> transformation documentation.</p>\n<p>Let’s say we have the following collection of Tuple2:</p>\n<pre><code>Tuple2&lt;Integer, String&gt; secondPerson = new Tuple2&lt;&gt;(4, &quot;Gautier&quot;);\nTuple2&lt;Integer, String&gt; thirdPerson = new Tuple2&lt;&gt;(5, &quot;Frédéric&quot;);\nTuple2&lt;Integer, String&gt; fourthPerson = new Tuple2&lt;&gt;(200, &quot;Cyrille&quot;);\nTuple2&lt;Integer, String&gt; firstPerson = new Tuple2&lt;&gt;(1, &quot;Charles&quot;);\nDataSet&lt;Tuple2&lt;Integer, String&gt;&gt; transactions = env.fromElements(fourthPerson, secondPerson, thirdPerson, firstPerson);\n</code></pre>\n<p>If we want to sort this collection by the first field of the tuple i.e. the id field, we’d do that with the <strong>sortPartitions()</strong> operator (and the IdKeySelectorTransaction we implemented a bit earlier):</p>\n<pre><code>List&lt;Tuple2&lt;Integer, String&gt;&gt; sorted = transactions\n    .sortPartition(new IdKeySelectorTransaction(), Order.ASCENDING)\n    .collect();\n</code></pre>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229237_-63681924","id":"paragraph_1564481864464_-336646410","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:39+0000","dateFinished":"2020-03-13T13:06:39+0000","status":"FINISHED","$$hashKey":"object:9581"},{"text":"%md\n###### e. FlatMap + GroupBy + Aggregate => Word Count\nThe Word Count problem is the “Hello World!” of the big data processing frameworks. It involves counting word occurrences of a given text input. \nTo do so, we will define a **LineSplitter** class, implementing the [FlatMapFunction interface](https://ci.apache.org/projects/flink/flink-docs-release-1.2/api/java/org/apache/flink/api/common/functions/FlatMapFunction.html). It has the following syntax:\n<center> **Interface FlatMapFunction`<T,O>`** </center>\n**Where:**\n- T - Type of the input elements\n- O - Type of the returned elements\n**We will give it:**\n- A String as an input (i.e. the whole raw text) and,\n- It will produce a collection of Tuple2<String,Integer> as an output (i.e. the (word, 1) Tuple2),\n\nOur **LineSplitter** class will be the following one:\n\n    class LineSplitter implements FlatMapFunction<String, Tuple2<String, Integer>> {\n        @Override\n        public void flatMap(String value, Collector<Tuple2<String, Integer>> out) {\n            Stream.of(value.toLowerCase().split(\"\\\\W+\"))\n                .filter(token -> token.length() > 0)\n                .forEach(token -> out.collect(new Tuple2<>(token, 1)));\n        }\n    }\n    \nOur next and final step is to **group()** the Tuple2 by their first elements (i.e. the words) and then perform a **sum()** aggregate on the second elements (i.e. the count that has been initialized to 1) to produce a count of all the word occurrences:\n\n    List<String> lines = Arrays.asList(\"This is a first sentence\", \"This is a second sentence with a one word\");\n    DataSet<String> text = env.fromCollection(lines);\n    DataSet<Tuple2<String,Integer>> counts = text.flatMap(new LineSplitter())\n        .groupBy(0)\n        .aggregate(Aggregations.SUM, 1);","user":"anonymous","dateUpdated":"2020-03-13T13:06:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h6>e. FlatMap + GroupBy + Aggregate =&gt; Word Count</h6>\n<p>The Word Count problem is the “Hello World!” of the big data processing frameworks. It involves counting word occurrences of a given text input.<br/>To do so, we will define a <strong>LineSplitter</strong> class, implementing the <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.2/api/java/org/apache/flink/api/common/functions/FlatMapFunction.html\">FlatMapFunction interface</a>. It has the following syntax:<br/><center> <strong>Interface FlatMapFunction<code>&lt;T,O&gt;</code></strong> </center><br/><strong>Where:</strong><br/>- T - Type of the input elements<br/>- O - Type of the returned elements<br/><strong>We will give it:</strong><br/>- A String as an input (i.e. the whole raw text) and,<br/>- It will produce a collection of Tuple2&lt;String,Integer&gt; as an output (i.e. the (word, 1) Tuple2),</p>\n<p>Our <strong>LineSplitter</strong> class will be the following one:</p>\n<pre><code>class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; {\n    @Override\n    public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) {\n        Stream.of(value.toLowerCase().split(&quot;\\\\W+&quot;))\n            .filter(token -&gt; token.length() &gt; 0)\n            .forEach(token -&gt; out.collect(new Tuple2&lt;&gt;(token, 1)));\n    }\n}\n</code></pre>\n<p>Our next and final step is to <strong>group()</strong> the Tuple2 by their first elements (i.e. the words) and then perform a <strong>sum()</strong> aggregate on the second elements (i.e. the count that has been initialized to 1) to produce a count of all the word occurrences:</p>\n<pre><code>List&lt;String&gt; lines = Arrays.asList(&quot;This is a first sentence&quot;, &quot;This is a second sentence with a one word&quot;);\nDataSet&lt;String&gt; text = env.fromCollection(lines);\nDataSet&lt;Tuple2&lt;String,Integer&gt;&gt; counts = text.flatMap(new LineSplitter())\n    .groupBy(0)\n    .aggregate(Aggregations.SUM, 1);\n</code></pre>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229237_998144645","id":"paragraph_1564481863394_939854638","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:39+0000","dateFinished":"2020-03-13T13:06:39+0000","status":"FINISHED","$$hashKey":"object:9582"},{"text":"%md\n#### 2.2. Streaming mode (DataStream API)\nIn streaming mode, we still have to give our Flink program its execution context. We will be using the [StreamExecutionEnvironment](https://ci.apache.org/projects/flink/flink-docs-stable/api/java/index.html?org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.html) class this time:\n\n    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();","user":"anonymous","dateUpdated":"2020-03-13T13:06:39+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>2.2. Streaming mode (DataStream API)</h4>\n<p>In streaming mode, we still have to give our Flink program its execution context. We will be using the <a href=\"https://ci.apache.org/projects/flink/flink-docs-stable/api/java/index.html?org/apache/flink/streaming/api/environment/StreamExecutionEnvironment.html\">StreamExecutionEnvironment</a> class this time:</p>\n<pre><code>StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n</code></pre>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229237_1813983346","id":"paragraph_1564481861888_-754784816","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:40+0000","dateFinished":"2020-03-13T13:06:40+0000","status":"FINISHED","$$hashKey":"object:9583"},{"text":"%md\n##### 2.2.1. Creating a DataStream\nNow, we will create a DataStream of events. Please have a look at the Flink’s [streaming connectors](https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/) documentation to check all the available connectors (e.g. Kafka, Cassandra, Elasticsearch, …).\nHere we will simply create a source from a couple of String elements:\n\n    DataStream<String> sentences = env.fromElements(\"This is a first sentence\", \"This is a second sentence with a one word\");","user":"anonymous","dateUpdated":"2020-03-13T13:06:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h5>2.2.1. Creating a DataStream</h5>\n<p>Now, we will create a DataStream of events. Please have a look at the Flink’s <a href=\"https://ci.apache.org/projects/flink/flink-docs-stable/dev/connectors/\">streaming connectors</a> documentation to check all the available connectors (e.g. Kafka, Cassandra, Elasticsearch, …).<br/>Here we will simply create a source from a couple of String elements:</p>\n<pre><code>DataStream&lt;String&gt; sentences = env.fromElements(&quot;This is a first sentence&quot;, &quot;This is a second sentence with a one word&quot;);\n</code></pre>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229238_-13895382","id":"paragraph_1564480916063_-2110112619","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:40+0000","dateFinished":"2020-03-13T13:06:40+0000","status":"FINISHED","$$hashKey":"object:9584"},{"text":"%md\n##### 2.2.2. Transformations\nWe can now perform the same transformations than the ones offered by the batch mode DataSet class (e.g. filter(), map(), reduce(), … ).  \nPlease have a look at the [DataStream transformations](https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/stream/operators/) documentation for more details.\nFor example let’s perform a **map()** transformation that will put our input sentences into capital letters:\n\n    SingleOutputStreamOperator<String> upperCaseSentences = sentences.map(String::toUpperCase);\n\nOf course, let’s not forget to perform a sink operation to trigger the computation:\n\n    upperCaseSentences.print();\n    env.execute();\n\nThat will output the following:\n\n    THIS IS A SECOND SENTENCE WITH A ONE WORD\n    THIS IS A FIRST SENTENCE","user":"anonymous","dateUpdated":"2020-03-13T13:06:40+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h5>2.2.2. Transformations</h5>\n<p>We can now perform the same transformations than the ones offered by the batch mode DataSet class (e.g. filter(), map(), reduce(), … ).<br/>Please have a look at the <a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.8/dev/stream/operators/\">DataStream transformations</a> documentation for more details.<br/>For example let’s perform a <strong>map()</strong> transformation that will put our input sentences into capital letters:</p>\n<pre><code>SingleOutputStreamOperator&lt;String&gt; upperCaseSentences = sentences.map(String::toUpperCase);\n</code></pre>\n<p>Of course, let’s not forget to perform a sink operation to trigger the computation:</p>\n<pre><code>upperCaseSentences.print();\nenv.execute();\n</code></pre>\n<p>That will output the following:</p>\n<pre><code>THIS IS A SECOND SENTENCE WITH A ONE WORD\nTHIS IS A FIRST SENTENCE\n</code></pre>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229238_-636536173","id":"paragraph_1564480915836_1335387101","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:41+0000","dateFinished":"2020-03-13T13:06:41+0000","status":"FINISHED","$$hashKey":"object:9585"},{"text":"%md\n##### 2.2.3. Windowing of events\nLet’s say we want to implement the following use-cases:\n\n- “Count the number of edits that are performed on our website every minute”\n- “Count how many edits were performed by each user every 10 minutes”\n\nThis is impossible using only the operators we presented. To tackle those use-cases we need to process group of events in a finite time-frame. This is where **windowing** comes in.\n\n**Types of windowing**\n\n| ![pic7](http://193.190.127.227/img/intro/pic7) | \n|:--:| \n| *Fig. 7: Types of windowing([Source](https://brewing.codes/2017/10/09/start-flink-streaming/): Brewing Codes).* |\n\n**Flink proposes 4 types of windowing:**\n\n- **Tumbling window:** creates non-overlapping adjacent windows in a stream. We can either group events by **time** (e.g. all events from 11:00 to 11:05) or by **count** (e.g. the first 50 events go into a separate group)\n- **Sliding window:** similar to the tumbling window but where windows can overlap. We could use it if we need to calculate a metric for the last 5 minutes, but we’d want to display some output every minute.\n- **Session window:** groups events that occurred close in time to each other.\n- **Global window:** puts all elements in a single window. Could be useful if we define a custom trigger that happens when a window is finished.\n\nIn addition to selecting how to assign events to different windows, we need to select a stream type. Flink proposes 2 types of DataStreams:\n\n| ![pic8](http://193.190.127.227/img/intro/pic8) | \n|:--:| \n| *Fig. 8: A Keyed stream([Source](https://brewing.codes/2017/10/09/start-flink-streaming/): Brewing Codes).* |\n\n- **Keyed stream:** where Flink partitions a single stream into multiple independent streams by some key (e.g. the name of a user). Working that way allows Flink to parallelize computation.\n- **Non-keyed stream:**  where all events in the stream will be processed together and our UDF (user-defined-function) will have access to all events within the stream. The downside is that it gives no parallelism and only one machine in the cluster will be able to execute our code.\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h5>2.2.3. Windowing of events</h5>\n<p>Let’s say we want to implement the following use-cases:</p>\n<ul>\n  <li>“Count the number of edits that are performed on our website every minute”</li>\n  <li>“Count how many edits were performed by each user every 10 minutes”</li>\n</ul>\n<p>This is impossible using only the operators we presented. To tackle those use-cases we need to process group of events in a finite time-frame. This is where <strong>windowing</strong> comes in.</p>\n<p><strong>Types of windowing</strong></p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic7\" alt=\"pic7\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 7: Types of windowing(<a href=\"https://brewing.codes/2017/10/09/start-flink-streaming/\">Source</a>: Brewing Codes).</em> </td>\n    </tr>\n  </tbody>\n</table>\n<p><strong>Flink proposes 4 types of windowing:</strong></p>\n<ul>\n  <li><strong>Tumbling window:</strong> creates non-overlapping adjacent windows in a stream. We can either group events by <strong>time</strong> (e.g. all events from 11:00 to 11:05) or by <strong>count</strong> (e.g. the first 50 events go into a separate group)</li>\n  <li><strong>Sliding window:</strong> similar to the tumbling window but where windows can overlap. We could use it if we need to calculate a metric for the last 5 minutes, but we’d want to display some output every minute.</li>\n  <li><strong>Session window:</strong> groups events that occurred close in time to each other.</li>\n  <li><strong>Global window:</strong> puts all elements in a single window. Could be useful if we define a custom trigger that happens when a window is finished.</li>\n</ul>\n<p>In addition to selecting how to assign events to different windows, we need to select a stream type. Flink proposes 2 types of DataStreams:</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic8\" alt=\"pic8\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 8: A Keyed stream(<a href=\"https://brewing.codes/2017/10/09/start-flink-streaming/\">Source</a>: Brewing Codes).</em> </td>\n    </tr>\n  </tbody>\n</table>\n<ul>\n  <li><strong>Keyed stream:</strong> where Flink partitions a single stream into multiple independent streams by some key (e.g. the name of a user). Working that way allows Flink to parallelize computation.</li>\n  <li><strong>Non-keyed stream:</strong> where all events in the stream will be processed together and our UDF (user-defined-function) will have access to all events within the stream. The downside is that it gives no parallelism and only one machine in the cluster will be able to execute our code.</li>\n</ul>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229238_208717701","id":"paragraph_1564480915279_2075691798","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:41+0000","dateFinished":"2020-03-13T13:06:41+0000","status":"FINISHED","$$hashKey":"object:9586"},{"text":"%md\n#### 3. Spark vs Flink\n##### 3.1. Spark computing framework\n\n| ![pic9](http://193.190.127.227/img/intro/pic9) | \n|:--:| \n| *Fig. 9: Spark computing framework([Source](http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/): datastrophic.io)* |\n\n- Though it has evolved through the years, Spark’s basic building block was originally **R**esilient **D**istributed **D**atasets (aka **RDDs** )\n- Spark uses transformations (operators) on RDDs to describe data processing\n- Together, all operators form a **D**irected **A**cyclic **G**raph (aka **DAG**)\n- This DAG is built during execution time via the DAGScheduler, that takes into account what transformations can be grouped together \n- Transformations that don’t require shuffling/repartitioning (= that can be parallelized together) are indeed grouped into stages (of tasks)\n- Tasks are then run on workers and results are returned\n","user":"anonymous","dateUpdated":"2020-03-13T13:11:08+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>3. Spark vs Flink</h4>\n<h5>3.1. Spark computing framework</h5>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic9\" alt=\"pic9\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 9: Spark computing framework(<a href=\"http://datastrophic.io/core-concepts-architecture-and-internals-of-apache-spark/\">Source</a>: datastrophic.io)</em> </td>\n    </tr>\n  </tbody>\n</table>\n<ul>\n  <li>Though it has evolved through the years, Spark’s basic building block was originally <strong>R</strong>esilient <strong>D</strong>istributed <strong>D</strong>atasets (aka <strong>RDDs</strong> )</li>\n  <li>Spark uses transformations (operators) on RDDs to describe data processing</li>\n  <li>Together, all operators form a <strong>D</strong>irected <strong>A</strong>cyclic <strong>G</strong>raph (aka <strong>DAG</strong>)</li>\n  <li>This DAG is built during execution time via the DAGScheduler, that takes into account what transformations can be grouped together</li>\n  <li>Transformations that don’t require shuffling/repartitioning (= that can be parallelized together) are indeed grouped into stages (of tasks)</li>\n  <li>Tasks are then run on workers and results are returned</li>\n</ul>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229238_-2114997675","id":"paragraph_1564480914630_429934895","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:11:08+0000","dateFinished":"2020-03-13T13:11:08+0000","status":"FINISHED","$$hashKey":"object:9587"},{"text":"%md\n##### 3.2. Flink computing framework\nSince its early days, Apache Flink has followed the philosophy of taking a **unified approach** to batch and streaming data processing. Indeed, Flink’s core building block is **“continuous processing of unbounded data streams”**: if you can do that, you can also do offline processing of bounded data sets (i.e. batch processing), because there are just bounded streams.\n\n| ![pic10](http://193.190.127.227/img/intro/pic10) | \n|:--:| \n| *Fig. 10: Type of streams([Source](https://flink.apache.org/news/2019/02/13/unified-batch-streaming-blink.html): Flink doc.)* |\n\n- To process data, Flink uses operators on data streams, with each operator generating a new data stream\n- In terms of **operators**, **chaining of operators** and **DAGs**, the **overall model** is **roughly equivalent to Spark’s**.\n- Flink’s vertices are roughly equivalent to stages in Spark, and dividing operators into vertices is basically the same as dividing stages in Spark DAG\n\n| ![pic11](http://193.190.127.227/img/intro/pic11) | \n|:--:| \n| *Fig. 11: Example of Flink’s chaining of operators([Source](https://ci.apache.org/projects/flink/flink-docs-stable/concepts/runtime.html#tasks-and-operator-chains): Flink doc.).* |\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:42+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h5>3.2. Flink computing framework</h5>\n<p>Since its early days, Apache Flink has followed the philosophy of taking a <strong>unified approach</strong> to batch and streaming data processing. Indeed, Flink’s core building block is <strong>“continuous processing of unbounded data streams”</strong>: if you can do that, you can also do offline processing of bounded data sets (i.e. batch processing), because there are just bounded streams.</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic10\" alt=\"pic10\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 10: Type of streams(<a href=\"https://flink.apache.org/news/2019/02/13/unified-batch-streaming-blink.html\">Source</a>: Flink doc.)</em> </td>\n    </tr>\n  </tbody>\n</table>\n<ul>\n  <li>To process data, Flink uses operators on data streams, with each operator generating a new data stream</li>\n  <li>In terms of <strong>operators</strong>, <strong>chaining of operators</strong> and <strong>DAGs</strong>, the <strong>overall model</strong> is <strong>roughly equivalent to Spark’s</strong>.</li>\n  <li>Flink’s vertices are roughly equivalent to stages in Spark, and dividing operators into vertices is basically the same as dividing stages in Spark DAG</li>\n</ul>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic11\" alt=\"pic11\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 11: Example of Flink’s chaining of operators(<a href=\"https://ci.apache.org/projects/flink/flink-docs-stable/concepts/runtime.html#tasks-and-operator-chains\">Source</a>: Flink doc.).</em> </td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229238_20839715","id":"paragraph_1564480706687_132997571","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:42+0000","dateFinished":"2020-03-13T13:06:42+0000","status":"FINISHED","$$hashKey":"object:9588"},{"text":"%md\n##### 3.3. Spark Word Count vs Flink Word Count\nWe wrote the exact same “Word Count” program in Spark (Java) and Flink (Java as well). Full code can be found at @PUTCODESOMEWHERE.\n\n**Operations:**\nThe operations performed in order to achieve to word count of a given file are the following:\n  \n  **Spark** Java Word count                         | **Flink** Java Word count (batch)\n  -------------                                     | -------------\n  **.readTextFile()** (//Read input file)           | **.textFile()** (//Read input file)\n  **.flatMap** (//Split up the lines)               | **.flatMap.collect** (//Splits up the lines in pairs (word, 1) and returns them)\n  **.mapToPair** (//Map the lines in pairs (word,1))| **.groupBy(0)** (//Groups by word)\n  **.reduceByKey** (//Sums the word occurrences)    | **.sum(1)** (//Sums the word occurrences)\n  **.collect** (//Gets all the counts)              | **.writeAsText** (//Emits the results)\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:42+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h5>3.3. Spark Word Count vs Flink Word Count</h5>\n<p>We wrote the exact same “Word Count” program in Spark (Java) and Flink (Java as well). Full code can be found at @PUTCODESOMEWHERE.</p>\n<p><strong>Operations:</strong><br/>The operations performed in order to achieve to word count of a given file are the following:</p>\n<table>\n  <thead>\n    <tr>\n      <th><strong>Spark</strong> Java Word count </th>\n      <th><strong>Flink</strong> Java Word count (batch)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td><strong>.readTextFile()</strong> (//Read input file) </td>\n      <td><strong>.textFile()</strong> (//Read input file)</td>\n    </tr>\n    <tr>\n      <td><strong>.flatMap</strong> (//Split up the lines) </td>\n      <td><strong>.flatMap.collect</strong> (//Splits up the lines in pairs (word, 1) and returns them)</td>\n    </tr>\n    <tr>\n      <td><strong>.mapToPair</strong> (//Map the lines in pairs (word,1))</td>\n      <td><strong>.groupBy(0)</strong> (//Groups by word)</td>\n    </tr>\n    <tr>\n      <td><strong>.reduceByKey</strong> (//Sums the word occurrences) </td>\n      <td><strong>.sum(1)</strong> (//Sums the word occurrences)</td>\n    </tr>\n    <tr>\n      <td><strong>.collect</strong> (//Gets all the counts) </td>\n      <td><strong>.writeAsText</strong> (//Emits the results)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229239_1065119401","id":"paragraph_1564480705471_1735131327","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:43+0000","dateFinished":"2020-03-13T13:06:43+0000","status":"FINISHED","$$hashKey":"object:9589"},{"text":"%md\n**DAGs:**\nResulting in the following DAGs:\n\n| ![pic12](http://193.190.127.227/img/intro/pic12) | \n|:--:| \n| *Fig. 12: Spark RDD Word Count DAG(Source: EURA NOVA).* |\n\n| ![pic13](http://193.190.127.227/img/intro/pic13) | \n|:--:| \n| *Fig. 13: Flink Word Count “DAG” (Source: EURA NOVA).* |\n\nAs one can notice, both the operators and the built DAG are indeed more or less similar in Spark and Flink (on a functional point of view).","user":"anonymous","dateUpdated":"2020-03-13T13:06:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>DAGs:</strong><br/>Resulting in the following DAGs:</p>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic12\" alt=\"pic12\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 12: Spark RDD Word Count DAG(Source: EURA NOVA).</em> </td>\n    </tr>\n  </tbody>\n</table>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic13\" alt=\"pic13\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 13: Flink Word Count “DAG” (Source: EURA NOVA).</em> </td>\n    </tr>\n  </tbody>\n</table>\n<p>As one can notice, both the operators and the built DAG are indeed more or less similar in Spark and Flink (on a functional point of view).</p>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229239_897044551","id":"paragraph_1564479566369_841158422","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:43+0000","dateFinished":"2020-03-13T13:06:43+0000","status":"FINISHED","$$hashKey":"object:9590"},{"text":"%md\n##### 3.4. Spark vs Flink: differences\n**To summarize:**\n\n- The **`key differences`** between Spark and Flink are the **`different computational concepts`** underlying each framework:\n    - **Spark** uses a **batch concept** for both batch and stream processing.\n    - **Flink**, on the other hand, is based on a **pure streaming approach**.\n    \n- Spark and Flink have **one significant difference** in **DAG execution**:\n    - **Spark**’s downstream stage starts processing the upstream’s output only after this last one is complete\n        - This unfortunately adds some **latency** to the processing time\n    \n    - **Flink**’s output of an event after processing one node can directly be sent to the next node for immediate processing\n\n- Another difference is the **state management** (if the result of processing an event is only related to the event itself, it is called stateless processing, otherwise - related to the previously processed event - it is called stateful processing):\n    - **Spark** did not originally provide built-in state support, as state management is a streaming concern (batch jobs only have inputs and outputs).\n        - According the [Discretized Streams paper](https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf) and [Drizzle paper](https://shivaram.org/publications/drizzle-sosp17.pdf), they explain that they can checkpoint state after each task. In this way, the failure recovery start from the last checkpoint and the last task. According to the [Drizzle paper](https://shivaram.org/publications/drizzle-sosp17.pdf), this is why they are more efficient in recovery but less efficient on normal conditions.\n        - This drawback has been mitigated with Spark Streaming which introduced a checkpoint mechanism.\n    \n    - **Flink** offers a checkpoint mechanism that periodically saves the application state in order to be able to recover it in case of failure.\n        - This means a failure will trigger a recovery of the previous state.\n\n| ![pic14](http://193.190.127.227/img/intro/pic14) | \n|:--:| \n| *Fig. 14: Flink Checkpointing and Stream Barriers([Source](https://ci.apache.org/projects/flink/flink-docs-release-1.7/internals/stream_checkpointing.html): Flink doc.).* |","user":"anonymous","dateUpdated":"2020-03-13T13:06:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"runOnSelectionChange":true,"title":false,"checkEmpty":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h5>3.4. Spark vs Flink: differences</h5>\n<p><strong>To summarize:</strong></p>\n<ul>\n  <li>\n    <p>The <strong><code>key differences</code></strong> between Spark and Flink are the <strong><code>different computational concepts</code></strong> underlying each framework:</p>\n    <ul>\n      <li><strong>Spark</strong> uses a <strong>batch concept</strong> for both batch and stream processing.</li>\n      <li><strong>Flink</strong>, on the other hand, is based on a <strong>pure streaming approach</strong>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>Spark and Flink have <strong>one significant difference</strong> in <strong>DAG execution</strong>:</p>\n    <ul>\n      <li>\n        <p><strong>Spark</strong>’s downstream stage starts processing the upstream’s output only after this last one is complete</p>\n        <ul>\n          <li>This unfortunately adds some <strong>latency</strong> to the processing time</li>\n        </ul>\n      </li>\n      <li>\n      <p><strong>Flink</strong>’s output of an event after processing one node can directly be sent to the next node for immediate processing</p></li>\n    </ul>\n  </li>\n  <li>\n    <p>Another difference is the <strong>state management</strong> (if the result of processing an event is only related to the event itself, it is called stateless processing, otherwise - related to the previously processed event - it is called stateful processing):</p>\n    <ul>\n      <li>\n        <p><strong>Spark</strong> did not originally provide built-in state support, as state management is a streaming concern (batch jobs only have inputs and outputs).</p>\n        <ul>\n          <li>According the <a href=\"https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf\">Discretized Streams paper</a> and <a href=\"https://shivaram.org/publications/drizzle-sosp17.pdf\">Drizzle paper</a>, they explain that they can checkpoint state after each task. In this way, the failure recovery start from the last checkpoint and the last task. According to the <a href=\"https://shivaram.org/publications/drizzle-sosp17.pdf\">Drizzle paper</a>, this is why they are more efficient in recovery but less efficient on normal conditions.</li>\n          <li>This drawback has been mitigated with Spark Streaming which introduced a checkpoint mechanism.</li>\n        </ul>\n      </li>\n      <li>\n        <p><strong>Flink</strong> offers a checkpoint mechanism that periodically saves the application state in order to be able to recover it in case of failure.</p>\n        <ul>\n          <li>This means a failure will trigger a recovery of the previous state.</li>\n        </ul>\n      </li>\n    </ul>\n  </li>\n</ul>\n<table>\n  <thead>\n    <tr>\n      <th align=\"center\"><img src=\"http://193.190.127.227/img/intro/pic14\" alt=\"pic14\" /> </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td align=\"center\"><em>Fig. 14: Flink Checkpointing and Stream Barriers(<a href=\"https://ci.apache.org/projects/flink/flink-docs-release-1.7/internals/stream_checkpointing.html\">Source</a>: Flink doc.).</em> </td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583824229239_974615163","id":"paragraph_1564479567235_-2042592151","dateCreated":"2020-03-10T07:10:29+0000","dateStarted":"2020-03-13T13:06:44+0000","dateFinished":"2020-03-13T13:06:44+0000","status":"FINISHED","$$hashKey":"object:9591"},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-03-13T13:06:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"progressUpdateIntervalMs":500,"jobName":"paragraph_1583825694612_-829607512","id":"paragraph_1583825694612_-829607512","dateCreated":"2020-03-10T07:34:54+0000","status":"FINISHED","$$hashKey":"object:9592"}],"name":"1. Flink Introduction","id":"2F367AVDE","defaultInterpreterGroup":"spark","version":"0.9.0-SNAPSHOT","permissions":{},"noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{"isRunning":false},"path":"/1. Flink Introduction"}