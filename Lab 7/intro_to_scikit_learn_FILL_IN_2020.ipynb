{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Basic introduction to Ski-Kit Learn: Analyzing Twitter tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Connecting to Twitter API and getting Twitter credentials:\n",
    "\n",
    "1. Create a Twitter account on https://twitter.com.\n",
    "2. Go to https://developer.twitter.com/en/apps and log in with your Twitter account.\n",
    "3. Click “Create an App” and fill in details of the application (See Minerva for a more detailed explanation).\n",
    "4. The application’s tokens and keys are available in the “Keys and Access Tokens” tab.\n",
    "\n",
    "\n",
    "To gather Tweets, we will use the [Tweepy](https://github.com/tweepy/tweepy) library. To install Tweepy, open the command prompt as an administrator and run the following pip command: **pip install tweepy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# Consumer keys and access tokens, used for OAuth\n",
    "consumer_key = <FILL_IN_AS_STRING>          \n",
    "consumer_secret = <FILL_IN_AS_STRING>    \n",
    "access_token = <FILL_IN_AS_STRING>            \n",
    "access_token_secret = <FILL_IN_AS_STRING>  \n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Calling the api\n",
    "api = tweepy.API(auth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can interact with the API. For this lab, we will deal with gathering and analyzing tweets from certain Twitter accounts.\n",
    "Statuses posted by a specified user can be collected with a [GET statuses/user_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline) request.\n",
    "This boils down to a Tweepy user_timeline method.\n",
    "Have a look at the [syntax](http://docs.tweepy.org/en/v3.5.0/api.html) and possibilities.\n",
    "Let's take a look at the latest 5 Tweets by @Bart_DeWever (a Belgian politician): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = '@Bart_DeWever'\n",
    "number_of_tweets = 5\n",
    "tweets = api.user_timeline(screen_name = handle, count = number_of_tweets)\n",
    "\n",
    "print(type(tweets))\n",
    "print(type(tweets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that tweets is a set of Tweepy **'Status' objects**. Take a look [here](https://gist.github.com/jaymcgrath/367c521f1dd786bc5a05ec3eeeb1cb04) to see all the attributes and subattributes of the object.\n",
    "For example, we can get the text and author's name of the latest tweet by using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[0].text)\n",
    "print(tweets[1].author.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Find the author's location. How many retweets did his latest post have? Check his Twitter page to verify your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = <FILL_IN>\n",
    "retweets = <FILL_IN>\n",
    "\n",
    "print('The author\\'s location is '+location)\n",
    "print('His latest post has '+repr(retweets)+' retweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retweets** can be excluded by adding the parameter '**include_rts = False**' to the user_timeline command. Note however that the count parameter still counts retweets, and hence usually len(tweets) < number_of_tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Out of his latest 10 posts, print all his original Tweets (= a Tweet that is not a retweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_tweets = 10\n",
    "#<FILL_IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. SciKit Learn and Machine Learning in practice: an introduction \n",
    "The **problem** is as follows: We want to analyze the latest tweets by the leaders of the Belgian political parties. Based on their chosen language, the topics they discuss, the hashtags etc., we would like to see if we can predict which Tweet belongs to who. __Text classification__ is a common Machine Learning task, and we expect that our problem should have a somewhat decent solution. Indeed, we can imagine a right-wing politician and a left-wing/green politican to tweet differently about ongoing events or political topics.\n",
    "\n",
    "First, install the following packages: **pip install sklearn**, **pip install nltk** and **pip install numpy**\n",
    "\n",
    "### 1. Bag-of-words Model\n",
    "In our model, each datapoint consists of the text of a Tweet and an associated label (= the Author's name). However, in order to run some generic black-box classifier, we need to convert this data to numeric values.\n",
    "The **labels** can easily be converted by just assigning an integer value to each Author. The text can be converted to numeric **features** by using the well-known [Bag-of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) model. Basically, we keep track of all the unique words in all the Tweets, and for each unique word, we count how many times it occurs in each Tweet.\n",
    "\n",
    "In scikit-learn, this is done by a **vectorizer**, see Section 4.2.3 [here](https://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation). Let's apply this to our corpus of Tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing packages.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# Twitter handles of the politicians.\n",
    "handles = ['@Bart_DeWever', '@conner_rousseau', '@RuttenGwendolyn', '@MeyremAlmaci', '@tomvangrieken']\n",
    "number_of_tweets = 200\n",
    "corpus = [] # a list of the text in all the tweets from the handles. \n",
    "labels = [] # a list with the numeric label of each tweet (1='@Bart_DeWever', 2='@wbeke', etc.)\n",
    "teller = 1\n",
    "\n",
    "# Getting the Tweets from the handles, excluding Retweets and getting full_text from the tweets.\n",
    "for name in handles:\n",
    "    tweets = api.user_timeline(screen_name = name, count = number_of_tweets, include_rts = False, tweet_mode=\"extended\")\n",
    "    corpus = corpus + [t.full_text for t in tweets] # getting the full text tweets.\n",
    "    labels = labels + [teller for i in range(0,len(tweets))]\n",
    "    teller +=1\n",
    "    \n",
    "# print(corpus[0:5]) # Printing the first tweets in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we convert this corpus to a numeric matrix X:\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#print(vectorizer.get_feature_names()) #__Checking the words that are vectorized, and thus part of the model.\n",
    "#print(X.shape) #__ Checking the dimension of the matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying generic Machine Learning classifiers\n",
    "Given our datapoints (the rows in X) and the labels, we are ready to run some basic classifiers. First, in order to evaluate our models properly, we need to split our data into **training data** and **test data**.\n",
    "\n",
    "**TASK**: use the 'train_test_split' function to split the data automatically. Make sure this is done in a **random** manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = <FILL_IN>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can train our models on the train data, and evaluate them on the test data. We will start by applying a [Decision Tree Classifier](https://scikit-learn.org/stable/modules/tree.html)  and a [Naive Bayes Classifier](https://scikit-learn.org/stable/modules/naive_bayes.html). To check **accuracy**, we simply count the number of times the classifier correctly predicted the label on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Decision Tree Classifier\n",
    "\n",
    "model_tree = tree.DecisionTreeClassifier(random_state=1) # Setting random_state = 1, to remove randomness in fitting.\n",
    "model_tree = model_tree.fit(X_train, y_train)\n",
    "y_predict = model_tree.predict(X_test)\n",
    "accuracy_tree = <FILL_IN>\n",
    "print('Decision Tree accuracy:'+'\\t'+repr(accuracy_tree))\n",
    "\n",
    "# Naive Bayes Classifier:\n",
    "\n",
    "model_bayes = MultinomialNB()\n",
    "model_bayes = model_bayes.fit(X_train, y_train)\n",
    "y_predict = model_bayes.predict(X_test)\n",
    "accuracy_bayes = <FILL_IN>\n",
    "print('Naive Bayes accuracy:'+'\\t'+repr(accuracy_bayes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a model, we will try to improve it. The first thing we can try, is to **redefine our corpus**.\n",
    "There are two problems with our current corpus:\n",
    "\n",
    "   a. There are many tweets with **urls** in them, which is clearly not informative in the classification.\n",
    "   \n",
    "   b. We have not removed the **stopwords** from the corpus (= very common words without much meaning).\n",
    "\n",
    "**TASK**: Make a new corpus, where the urls + Dutch stopwords are removed. Test the difference in performance of the classifiers. Explain why the performance, at first sight, might not have improved.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Dutch stop words from the NLTK repository.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Removing the links from the tweets (starting with https:// until a space)\n",
    "# HINT: iterate over the corpus, and use re.sub function to replace 'https' pieces with an empty string ''. \n",
    "corpus_no_url = <FILL_IN>\n",
    "    \n",
    "## Removing stopwords from Dutch language\n",
    "# HINT: Split the words in a tweet, only keep words that are not in stopWords, then join the seperate words into a string.\n",
    "stopWords = set(stopwords.words('dutch'))\n",
    "corpus_no_stops_no_url = <FILL_IN>\n",
    "\n",
    "# print(corpus_no_stops_no_url[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance testing with the new corpus\n",
    "<FILL_IN>\n",
    "\n",
    "print('Decision Tree accuracy with new corpus:'+'\\t'+<FILL_IN>)\n",
    "\n",
    "print('Naive Bayes accuracy: with new corpus'+'\\t'+<FILL_IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a couple of reasons why the accuracy might not be higher than with the original corpus:\n",
    "<FILL_IN>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
